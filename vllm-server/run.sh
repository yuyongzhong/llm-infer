#!/bin/bash

# =============================================================================
# vLLM æœåŠ¡å¯åŠ¨è„šæœ¬
# æ”¯æŒå•æœºå’Œå¤šæœºåˆ†å¸ƒå¼éƒ¨ç½²ï¼ŒåŒ…å«å®Œæ•´çš„å‚æ•°é…ç½®å’Œæ€§èƒ½ä¼˜åŒ–é€‰é¡¹
# =============================================================================

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# =============================================================================
# é»˜è®¤é…ç½®å‚æ•°
# =============================================================================

# åŸºç¡€å‚æ•°
TP_SIZE=""                              # å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆå¿…éœ€ï¼‰
PP_SIZE=""                              # æµæ°´çº¿å¹¶è¡Œå¤§å°ï¼ˆå¿…éœ€ï¼‰
MODEL_PATH=""                           # æ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
MODEL_NAME="deepseek"                   # æ¨¡å‹æœåŠ¡åç§°
PORT=8000                               # æœåŠ¡ç«¯å£

# èµ„æºé…ç½®
MUSA_VISIBLE_DEVICES="0,1,2,3,4,5,6,7" # å¯è§GPUè®¾å¤‡
MAX_MODEL_LEN=12000                     # æœ€å¤§æ¨¡å‹é•¿åº¦
BATCH_SIZE=128                          # æ‰¹å¤„ç†å¤§å°
GPU_MEMORY_UTILIZATION=0.8              # GPUå†…å­˜åˆ©ç”¨ç‡
BLOCK_SIZE=16                           # å†…å­˜å—å¤§å°

# åˆ†å¸ƒå¼é…ç½®
HOSTFILE=""                             # å¤šæœºé…ç½®æ–‡ä»¶
SSH_PORT=62262                          # SSHç«¯å£
RAY_PORT=62379                          # Rayç«¯å£
VLLM_PP_LAYER_PARTITION=""              # æµæ°´çº¿å±‚åˆ†åŒº

# æ€§èƒ½ä¼˜åŒ–å‚æ•°ï¼ˆå¯é€‰ï¼‰
MAX_NUM_SEQS=""                         # æœ€å¤§åºåˆ—æ•°ï¼ˆä¸ºç©ºæ—¶ä½¿ç”¨BATCH_SIZEï¼‰
ENABLE_CHUNKED_PREFILL=""               # å¯ç”¨åˆ†å—é¢„å¡«å…… (true/false)
MAX_NUM_BATCHED_TOKENS=""               # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
ENABLE_PREFIX_CACHING=""                # å¯ç”¨å‰ç¼€ç¼“å­˜ (true/false)
COMPILATION_CONFIG='{"cudagraph_capture_sizes": [1,2,3,4,5,6,7,8,10,12,14,16,18,20,24,28,30,32,64,128], "simple_cuda_graph": true}'  # ç¼–è¯‘é…ç½®

# =============================================================================
# å¸®åŠ©ä¿¡æ¯å‡½æ•°
# =============================================================================

show_help() {
    cat << 'EOF'
vLLM æœåŠ¡å¯åŠ¨è„šæœ¬

ç”¨æ³•:
    run.sh --tp <TP_SIZE> --pp <PP_SIZE> --model-path <MODEL_PATH> [OPTIONS]

å¿…éœ€å‚æ•°:
    --tp, --tensor-parallel-size    å¼ é‡å¹¶è¡Œå¤§å°
    --pp, --pipeline-parallel-size  æµæ°´çº¿å¹¶è¡Œå¤§å°
    --model-path                    æ¨¡å‹è·¯å¾„

åŸºç¡€é…ç½®:
    --model-name                    æ¨¡å‹æœåŠ¡åç§° (é»˜è®¤: deepseek)
    --port                          æœåŠ¡ç«¯å£ (é»˜è®¤: 8000)
    --visible-devices               å¯è§GPUè®¾å¤‡ (é»˜è®¤: 0,1,2,3,4,5,6,7)
    --musa-visible-devices          MUSAå¯è§GPUè®¾å¤‡ï¼Œè¦†ç›–é»˜è®¤å€¼

æ€§èƒ½å‚æ•°:
    --max-model-len                 æœ€å¤§æ¨¡å‹é•¿åº¦ (é»˜è®¤: 12000)
    --batch-size                    æ‰¹å¤„ç†å¤§å° (é»˜è®¤: 128)
    --gpu-memory-utilization        GPUå†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.8)
    --block-size                    å†…å­˜å—å¤§å° (é»˜è®¤: 16)
    --max-num-seqs                  æœ€å¤§åºåˆ—æ•°
    --max-num-batched-tokens        æœ€å¤§æ‰¹å¤„ç†tokenæ•°

ä¼˜åŒ–é€‰é¡¹:
    --enable-chunked-prefill        å¯ç”¨åˆ†å—é¢„å¡«å…… (true/false)
    --enable-prefix-caching         å¯ç”¨å‰ç¼€ç¼“å­˜ (true/false)
    --compilation-config            ç¼–è¯‘é…ç½®JSONå­—ç¬¦ä¸²

åˆ†å¸ƒå¼é…ç½®:
    --hostfile                      å¤šæœºé…ç½®æ–‡ä»¶
    --ssh-port                      SSHç«¯å£ (é»˜è®¤: 62262)
    --ray-port                      Rayç«¯å£ (é»˜è®¤: 62379)
    --partition                     æµæ°´çº¿å±‚åˆ†åŒº

ç¤ºä¾‹:
    # åŸºç¡€å•æœºå¯åŠ¨
    run.sh --tp 2 --pp 1 --model-path /path/to/model

    # é«˜çº§æ€§èƒ½ä¼˜åŒ–
    run.sh --tp 4 --pp 1 --model-path /path/to/model \
           --enable-chunked-prefill true \
           --enable-prefix-caching true \
           --compilation-config '{"simple_cuda_graph": true}'

    # æŒ‡å®šGPUè®¾å¤‡
    run.sh --tp 2 --pp 1 --model-path /path/to/deepseek \
           --musa-visible-devices "0,1"

    run.sh --tp 4 --pp 1 --model-path /path/to/qwen \
           --musa-visible-devices "2,3,4,5"

    # å¤šæœºåˆ†å¸ƒå¼
    run.sh --tp 8 --pp 2 --model-path /path/to/model \
           --hostfile hosts.txt \
           --distributed-executor-backend ray

EOF
}

# =============================================================================
# å‚æ•°è§£æ
# =============================================================================
while [[ $# -gt 0 ]]; do
  case $1 in
    # å¿…éœ€å‚æ•°
    --tp|--tensor-parallel-size)
      TP_SIZE="$2"; shift 2;;
    --pp|--pipeline-parallel-size)
      PP_SIZE="$2"; shift 2;;
    --model-path)
      MODEL_PATH="$2"; shift 2;;
    
    # åŸºç¡€é…ç½®
    --model-name)
      MODEL_NAME="$2"; shift 2;;
    --port)
      PORT="$2"; shift 2;;
    --visible-devices)
      MUSA_VISIBLE_DEVICES="$2"; shift 2;;
    --musa-visible-devices)
      MUSA_VISIBLE_DEVICES="$2"; shift 2;;
    
    # æ€§èƒ½å‚æ•°
    --max-model-len)
      MAX_MODEL_LEN="$2"; shift 2;;
    --batch-size)
      BATCH_SIZE="$2"; shift 2;;
    --gpu-memory-utilization)
      GPU_MEMORY_UTILIZATION="$2"; shift 2;;
    --block-size)
      BLOCK_SIZE="$2"; shift 2;;
    --max-num-seqs)
      MAX_NUM_SEQS="$2"; shift 2;;
    --max-num-batched-tokens)
      MAX_NUM_BATCHED_TOKENS="$2"; shift 2;;
    
    # ä¼˜åŒ–é€‰é¡¹
    --enable-chunked-prefill)
      ENABLE_CHUNKED_PREFILL="$2"; shift 2;;
    --enable-prefix-caching)
      ENABLE_PREFIX_CACHING="$2"; shift 2;;
    --compilation-config)
      COMPILATION_CONFIG="$2"; shift 2;;
    
    # åˆ†å¸ƒå¼é…ç½®
    --hostfile)
      HOSTFILE="$2"; shift 2;;
    --ssh-port)
      SSH_PORT="$2"; shift 2;;
    --ray-port)
      RAY_PORT="$2"; shift 2;;
    --partition)
      VLLM_PP_LAYER_PARTITION="$2"; shift 2;;
    
    # å…¼å®¹æ€§åˆ«åï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨æ ‡å‡†å‚æ•°åï¼‰
    --tensor-parallel-size)
      TP_SIZE="$2"; shift 2;;
    --pipeline-parallel-size)
      PP_SIZE="$2"; shift 2;;
    
    # å…¼å®¹æ€§åˆ«åï¼ˆå·²åºŸå¼ƒï¼Œå»ºè®®ä½¿ç”¨æ ‡å‡†å‚æ•°åï¼‰
    --tensor-parallel-size)
      TP_SIZE="$2"; shift 2;;
    --pipeline-parallel-size)
      PP_SIZE="$2"; shift 2;;
    
    --help|-h)
      show_help; exit 0;;
    *)
      echo "âŒ æœªçŸ¥å‚æ•°: $1"
      echo "ä½¿ç”¨ --help æŸ¥çœ‹ç”¨æ³•"
      exit 1;;
  esac
done

# =============================================================================
# å‚æ•°éªŒè¯
# =============================================================================

validate_parameters() {
    local errors=()
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    [[ -z "$TP_SIZE" ]] && errors+=("ç¼ºå°‘å¿…éœ€å‚æ•°: --tp")
    [[ -z "$PP_SIZE" ]] && errors+=("ç¼ºå°‘å¿…éœ€å‚æ•°: --pp")
    [[ -z "$MODEL_PATH" ]] && errors+=("ç¼ºå°‘å¿…éœ€å‚æ•°: --model-path")
    
    # æ£€æŸ¥æ•°å€¼å‚æ•°
    [[ ! "$TP_SIZE" =~ ^[0-9]+$ ]] && errors+=("--tp å¿…é¡»æ˜¯æ­£æ•´æ•°")
    [[ ! "$PP_SIZE" =~ ^[0-9]+$ ]] && errors+=("--pp å¿…é¡»æ˜¯æ­£æ•´æ•°")
    [[ ! "$PORT" =~ ^[0-9]+$ ]] && errors+=("--port å¿…é¡»æ˜¯æ­£æ•´æ•°")
    
    # æ£€æŸ¥æ¨¡å‹è·¯å¾„
    [[ ! -d "$MODEL_PATH" ]] && errors+=("æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $MODEL_PATH")
    
    if [[ ${#errors[@]} -gt 0 ]]; then
        echo "âŒ å‚æ•°éªŒè¯å¤±è´¥:"
        printf "   %s\n" "${errors[@]}"
        echo "ä½¿ç”¨ --help æŸ¥çœ‹ç”¨æ³•"
        exit 1
    fi
}

validate_parameters

# =============================================================================
# è®¡ç®—æ´¾ç”Ÿå‚æ•°
# =============================================================================

NUM_GPU_BLOCKS=$(( MAX_MODEL_LEN * BATCH_SIZE ))
WORLD_SIZE=$((PP_SIZE * TP_SIZE))

echo "ğŸ“Š é…ç½®ä¿¡æ¯:"
echo "   æ¨¡å‹è·¯å¾„: $MODEL_PATH"
echo "   å¼ é‡å¹¶è¡Œ: $TP_SIZE, æµæ°´çº¿å¹¶è¡Œ: $PP_SIZE"
echo "   æ€»GPUæ•°: $WORLD_SIZE, æœåŠ¡ç«¯å£: $PORT"
echo "   æœ€å¤§æ¨¡å‹é•¿åº¦: $MAX_MODEL_LEN, æ‰¹å¤„ç†å¤§å°: $BATCH_SIZE"

# =============================================================================
# ç¯å¢ƒé…ç½®
# =============================================================================

setup_environment() {
    echo "ğŸ”§ é…ç½®è¿è¡Œç¯å¢ƒ..."
    
    # ç¡®å®šè¿è¡Œæ¨¡å¼
    if [[ -z "$HOSTFILE" || ! -f "$HOSTFILE" ]]; then
        echo "ğŸ’» å•æœºæ¨¡å¼"
        USE_SINGLE_NODE=true
    else
        echo "ğŸŒ å¤šæœºæ¨¡å¼ï¼Œé…ç½®æ–‡ä»¶: $HOSTFILE"
        USE_SINGLE_NODE=false
    fi
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    local env_vars=(
        MCCL_PROTOS=2
        MUSA_PRINT_ENV=1
        MTHREADS_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
        MUSA_HOME="/usr/local/musa"
        TRITON_CACHE_DIR="/tmp/triton"
        LIBRARY_PATH="/opt/intel/oneapi/mkl/lib/intel64:${LIBRARY_PATH}"
        LD_LIBRARY_PATH="/usr/lib/x86_64-linux-gnu/:/usr/local/musa/lib"
        VLLM_NCCL_SO_PATH="/usr/local/musa/lib/libmccl.so.2"
        VLLM_TORCH_PROFILER_DIR="/home/model/"
        GLOO_SOCKET_IFNAME=bond0
        TP_SOCKET_IFNAME=bond0
        MUSA_VISIBLE_DEVICES=$MUSA_VISIBLE_DEVICES
        VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE=shm
        RAY_CGRAPH_get_timeout=3000
    )
    
    # æ·»åŠ å¯é€‰ç¯å¢ƒå˜é‡
    [[ -n "$VLLM_PP_LAYER_PARTITION" ]] && env_vars+=(VLLM_PP_LAYER_PARTITION="$VLLM_PP_LAYER_PARTITION")
    
    # å¯¼å‡ºç¯å¢ƒå˜é‡
    for var in "${env_vars[@]}"; do
        echo "export $var"
        eval "export $var"
    done
    
    # æ¸…ç†ä¹‹å‰çš„è¿›ç¨‹
    echo "ğŸ§¹ æ¸…ç†ç¯å¢ƒ..."
    pkill -f /opt/conda/envs/py310/bin/python3 || true
    ray stop || true
    rm -rf /tmp/triton/* || true
}

setup_environment

# =============================================================================
# æ—¥å¿—å’Œè¾“å‡ºç›®å½•è®¾ç½®
# =============================================================================

setup_logging() {
    CURRENT_TIME=$(date "+%Y-%m-%d_%H:%M:%S")
    WORK_HOME="$PWD"
    EXPNAME="pp${PP_SIZE}_tp${TP_SIZE}_gpus${WORLD_SIZE}"
    LOG_FILE="$WORK_HOME/output/$CURRENT_TIME/$EXPNAME.log"
    
    echo "ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: $WORK_HOME/output/$CURRENT_TIME"
    mkdir -p "$WORK_HOME/output/$CURRENT_TIME"
    
    echo "ğŸ“ æ—¥å¿—æ–‡ä»¶: $LOG_FILE"
}

setup_logging

# =============================================================================
# åˆ†å¸ƒå¼é›†ç¾¤è®¾ç½®
# =============================================================================

setup_distributed_cluster() {
    [[ "$USE_SINGLE_NODE" = true ]] && return 0
    
    echo "ğŸš€ è®¾ç½®åˆ†å¸ƒå¼é›†ç¾¤..."
    
    # è¯»å–ä¸»æœºåˆ—è¡¨
    mapfile -t hostlist < <(grep -v '^#\|^$' "$HOSTFILE" | awk '{print $1}' | tr -d '\r')
    
    # å‡†å¤‡ç¯å¢ƒå˜é‡æ•°ç»„
    env_array=(
        "MCCL_PROTOS=2"
        "MUSA_PRINT_ENV=1" 
        "MTHREADS_VISIBLE_DEVICES=0,1,2,3,4,5,6,7"
        "MUSA_HOME=/usr/local/musa"
        "TRITON_CACHE_DIR=/tmp/triton"
        "LIBRARY_PATH=/opt/intel/oneapi/mkl/lib/intel64:"
        "LD_LIBRARY_PATH=/usr/lib/x86_64-linux-gnu/:/usr/local/musa/lib"
        "VLLM_NCCL_SO_PATH=/usr/local/musa/lib/libmccl.so.2"
        "VLLM_TORCH_PROFILER_DIR=/home/model/"
        "GLOO_SOCKET_IFNAME=bond0"
        "TP_SOCKET_IFNAME=bond0"
        "MUSA_VISIBLE_DEVICES=${MUSA_VISIBLE_DEVICES}"
        "VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE=shm"
        "RAY_CGRAPH_get_timeout=3000"
    )
    
    first_host=true
    first_host_ip=""
    
    for host in "${hostlist[@]}"; do
        echo "ğŸ”— å¤„ç†ä¸»æœº: $host"
        
        # åœæ­¢ç°æœ‰Rayè¿›ç¨‹
        ssh -p "$SSH_PORT" "$host" "ray stop" || true
        
        if [ "$first_host" = "true" ]; then
            first_host=false
            first_host_ip="$host"
            echo "   å¯åŠ¨å¤´èŠ‚ç‚¹..."
            ssh -p "$SSH_PORT" "$host" "${env_array[@]}" ray start --head --port="${RAY_PORT}" --dashboard-host=0.0.0.0 --num-gpus=8
        else
            echo "   åŠ å…¥é›†ç¾¤..."
            ssh -p "$SSH_PORT" "$host" "${env_array[@]}" ray start --address "${first_host_ip}:${RAY_PORT}" --num-gpus=8
        fi
        
        echo "   âœ… ä¸»æœº $host å®Œæˆ"
    done
    
    echo "ğŸ” æ£€æŸ¥é›†ç¾¤çŠ¶æ€:"
    ray status
}

setup_distributed_cluster

# =============================================================================
# vLLM å‘½ä»¤æ„å»º
# =============================================================================

build_vllm_command() {
    echo "ğŸ”¨ æ„å»ºvLLMæœåŠ¡å‘½ä»¤..."
    
    local cmd="vllm serve \"$MODEL_PATH\""
    
    # åŸºç¡€å‚æ•°
    cmd="$cmd --trust-remote-code"  # é»˜è®¤å¯ç”¨
    cmd="$cmd --max_model_len $MAX_MODEL_LEN"
    cmd="$cmd --gpu-memory-utilization $GPU_MEMORY_UTILIZATION"
    cmd="$cmd --served-model-name $MODEL_NAME"
    cmd="$cmd --port $PORT"
    
    # å¹¶è¡Œé…ç½®
    cmd="$cmd -tp $TP_SIZE"
    cmd="$cmd -pp $PP_SIZE"
    
    # åºåˆ—å’Œæ‰¹å¤„ç†é…ç½®
    if [[ -n "$MAX_NUM_SEQS" && "$MAX_NUM_SEQS" != "" ]]; then
        cmd="$cmd --max-num-seqs $MAX_NUM_SEQS"
    else
        cmd="$cmd --max-num-seqs $BATCH_SIZE"
    fi
    
    # å¯é€‰çš„åŸºç¡€å‚æ•°
    [[ -n "$BLOCK_SIZE" && "$BLOCK_SIZE" != "" ]] && cmd="$cmd --block-size $BLOCK_SIZE"
    
    # æ€§èƒ½ä¼˜åŒ–å‚æ•°
    [[ "$ENABLE_CHUNKED_PREFILL" = "true" ]] && cmd="$cmd --enable-chunked-prefill"
    [[ "$ENABLE_PREFIX_CACHING" = "true" ]] && cmd="$cmd --enable-prefix-caching"
    [[ -n "$MAX_NUM_BATCHED_TOKENS" && "$MAX_NUM_BATCHED_TOKENS" != "" ]] && cmd="$cmd --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS"
    
    # ç¼–è¯‘é…ç½®
    if [[ -n "$COMPILATION_CONFIG" && "$COMPILATION_CONFIG" != "" ]]; then
        cmd="$cmd --compilation-config '$COMPILATION_CONFIG'"
    fi
    
    # åˆ†å¸ƒå¼æ‰§è¡Œåç«¯ï¼ˆå¤šæœºæ—¶ä½¿ç”¨Rayï¼‰
    if [[ "$USE_SINGLE_NODE" = false ]]; then
        cmd="$cmd --distributed-executor-backend ray"
    fi
    
    VLLM_CMD="$cmd"
}

build_vllm_command

# =============================================================================
# å¯åŠ¨ vLLM æœåŠ¡
# =============================================================================

echo "ğŸš€ å¯åŠ¨vLLMæœåŠ¡..."
echo "ğŸ§ æœ€ç»ˆå‘½ä»¤:"
echo "$VLLM_CMD"
echo ""

eval "$VLLM_CMD" 2>&1 | tee -a "$LOG_FILE"
