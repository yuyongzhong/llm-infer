{
  "project_id": "yyz_test_20250723_1000",
  "timestamp": "2025-07-23T016:30:15.593620Z",
  "model_name": "DeepSeek",
  "run_mode": "acc-then-bench",
  "base_info": {
    "device": "AITiansi-16G",
    "model": "gptq-DeepSeek-R1-Distill-Qwen-7B",
    "version": "1.0.1",
    "remark": "天思盒子",
    "software_version":"",
    "software_url": "",
    "vllm_image_harbor": {
      "huoshan_harbor_url": "",
      "shanghai_harbor_url": ""
    }
  },
  "tests": [
    {
      "test_type": "benchmark",
      "results": [
        {
          "timestamp": 0,
          "concurrency": 1,
          "input_tokens": 32,
          "output_tokens": 32,
          "expected_input": 32,
          "expected_output": 32,
          "avg_input": 32,
          "avg_output": 32,
          "total_time": 4.26,
          "req_throughput": 0.24,
          "output_throughput": 7.52,
          "total_throughput": 15.04,
          "avg_ttft": 494.83,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 2,
          "input_tokens": 32,
          "output_tokens": 32,
          "expected_input": 32,
          "expected_output": 32,
          "avg_input": 32,
          "avg_output": 32,
          "total_time": 2.41,
          "req_throughput": 0.83,
          "output_throughput": 13.29,
          "total_throughput": 26.58,
          "avg_ttft": 534.15,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 3,
          "input_tokens": 32,
          "output_tokens": 32,
          "expected_input": 32,
          "expected_output": 32,
          "avg_input": 32,
          "avg_output": 32,
          "total_time": 1.81,
          "req_throughput": 1.66,
          "output_throughput": 17.72,
          "total_throughput": 35.44,
          "avg_ttft": 569.48,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 4,
          "input_tokens": 32,
          "output_tokens": 32,
          "expected_input": 32,
          "expected_output": 32,
          "avg_input": 32,
          "avg_output": 32,
          "total_time": 1.10,
          "req_throughput": 3.63,
          "output_throughput": 29,
          "total_throughput": 58.02,
          "avg_ttft": 552.76,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 1,
          "input_tokens": 64,
          "output_tokens": 64,
          "expected_input": 64,
          "expected_output": 64,
          "avg_input": 64,
          "avg_output": 64,
          "total_time": 8.08,
          "req_throughput": 0.12,
          "output_throughput": 7.92,
          "total_throughput": 15.84,
          "avg_ttft": 541.61,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 2,
          "input_tokens": 64,
          "output_tokens": 64,
          "expected_input": 64,
          "expected_output": 64,
          "avg_input": 64,
          "avg_output": 64,
          "total_time": 4.61,
          "req_throughput": 0.43,
          "output_throughput": 13.87,
          "total_throughput": 27.74,
          "avg_ttft": 609.71,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 3,
          "input_tokens": 64,
          "output_tokens": 64,
          "expected_input": 64,
          "expected_output": 64,
          "avg_input": 64,
          "avg_output": 64,
          "total_time": 3.60,
          "req_throughput": 0.83,
          "output_throughput": 17.77,
          "total_throughput": 35.54,
          "avg_ttft": 709.73,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 4,
          "input_tokens": 64,
          "output_tokens": 64,
          "expected_input": 64,
          "expected_output": 64,
          "avg_input": 64,
          "avg_output": 64,
          "total_time": 3.49,
          "req_throughput": 1.15,
          "output_throughput": 18.33,
          "total_throughput": 36.66,
          "avg_ttft": 5890.81,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 1,
          "input_tokens": 128,
          "output_tokens": 128,
          "expected_input": 128,
          "expected_output": 128,
          "avg_input": 128,
          "avg_output": 128,
          "total_time": 16.22,
          "req_throughput": 0.06,
          "output_throughput": 7.89,
          "total_throughput": 15.78,
          "avg_ttft": 674.58,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 2,
          "input_tokens": 128,
          "output_tokens": 128,
          "expected_input": 128,
          "expected_output": 128,
          "avg_input": 128,
          "avg_output": 128,
          "total_time": 10.17,
          "req_throughput": 0.20,
          "output_throughput": 12.59,
          "total_throughput": 25.18,
          "avg_ttft": 1186.79,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 3,
          "input_tokens": 128,
          "output_tokens": 128,
          "expected_input": 128,
          "expected_output": 128,
          "avg_input": 128,
          "avg_output": 128,
          "total_time": 8.04,
          "req_throughput": 0.37,
          "output_throughput": 15.92,
          "total_throughput": 31.84,
          "avg_ttft": 1421.73,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 4,
          "input_tokens": 128,
          "output_tokens": 128,
          "expected_input": 128,
          "expected_output": 128,
          "avg_input": 128,
          "avg_output": 128,
          "total_time": 6.00,
          "req_throughput": 0.67,
          "output_throughput": 21.37,
          "total_throughput": 42.64,
          "avg_ttft": 6826.66,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 1,
          "input_tokens": 256,
          "output_tokens": 256,
          "expected_input": 256,
          "expected_output": 256,
          "avg_input": 256,
          "avg_output": 256,
          "total_time": 35.70,
          "req_throughput": 0.03,
          "output_throughput": 7.17,
          "total_throughput": 14.34,
          "avg_ttft": 1503.53,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 2,
          "input_tokens": 256,
          "output_tokens": 256,
          "expected_input": 256,
          "expected_output": 256,
          "avg_input": 256,
          "avg_output": 256,
          "total_time": 22.03,
          "req_throughput": 0.09,
          "output_throughput": 11.62,
          "total_throughput": 23.24,
          "avg_ttft": 2456.67,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 3,
          "input_tokens": 256,
          "output_tokens": 256,
          "expected_input": 256,
          "expected_output": 256,
          "avg_input": 256,
          "avg_output": 256,
          "total_time": 17.70,
          "req_throughput": 0.17,
          "output_throughput": 14.46,
          "total_throughput": 28.92,
          "avg_ttft": 3626.99,
          "p99_ttft": 0
        },
        {
          "timestamp": 0,
          "concurrency": 4,
          "input_tokens": 256,
          "output_tokens": 256,
          "expected_input": 256,
          "expected_output": 256,
          "avg_input": 256,
          "avg_output": 256,
          "total_time": 11.49,
          "req_throughput": 0.35,
          "output_throughput": 22.29,
          "total_throughput": 44.58,
          "avg_ttft": 9443.80,
          "p99_ttft": 0
        }
      ],
      "log_path": ""
    },
    {
    "test_type": "accuracy",
    "datasets": [
        {
            "name": "AIModel_v1.0.1_tiansihezi",
            "dataset_name": "ceval",
            "dataset_pretty_name": "C-Eval",
            "dataset_description": null,
            "model_name": "ceval",
            "score": 0.4,
            "metrics": [
                {
                    "name": "AverageAccuracy",
                    "num": 1346,
                    "score": 0.4,
                    "macro_score": 0.4075,
                    "categories": [
                        {
                            "name": ["Humanities"],
                            "num": 257,
                            "score": 0.396,
                            "macro_score": 0.396,
                            "subsets": [
                                {"name": "modern_chinese_history", "score": 0.43, "num": 23},
                                {"name": "ideological_and_moral_cultivation", "score": 0.58, "num": 19},
                                {"name": "logic", "score": 0.18, "num": 22},
                                {"name": "law", "score": 0.38, "num": 24},
                                {"name": "chinese_language_and_literature", "score": 0.3, "num": 23},
                                {"name": "art_studies", "score": 0.42, "num": 33},
                                {"name": "professional_tour_guide", "score": 0.48, "num": 29},
                                {"name": "legal_professional", "score": 0.17, "num": 23},
                                {"name": "high_school_chinese", "score": 0.32, "num": 19},
                                {"name": "high_school_history", "score": 0.65, "num": 20},
                                {"name": "middle_school_history", "score": 0.45, "num": 22}
                            ]
                        },
                        {
                            "name": ["Other"],
                            "num": 384,
                            "score": 0.39,
                            "macro_score": 0.39,
                            "subsets": [
                                {"name": "civil_servant", "score": 0.3, "num": 47},
                                {"name": "sports_science", "score": 0.42, "num": 19},
                                {"name": "plant_protection", "score": 0.64, "num": 22},
                                {"name": "basic_medicine", "score": 0.37, "num": 19},
                                {"name": "clinical_medicine", "score": 0.45, "num": 22},
                                {"name": "urban_and_rural_planner", "score": 0.39, "num": 46},
                                {"name": "accountant", "score": 0.29, "num": 49},
                                {"name": "fire_engineer", "score": 0.29, "num": 31},
                                {"name": "environmental_impact_assessment_engineer", "score": 0.39, "num": 31},
                                {"name": "tax_accountant", "score": 0.29, "num": 49},
                                {"name": "physician", "score": 0.37, "num": 49}
                            ]
                        },
                        {
                            "name": ["STEM"],
                            "num": 485,
                            "score": 0.3915,
                            "macro_score": 0.3915,
                            "subsets": [
                                {"name": "electrical_engineer", "score": 0.22, "num": 37},
                                {"name": "metrology_engineer", "score": 0.46, "num": 24},
                                {"name": "high_school_mathematics", "score": 0.22, "num": 18},
                                {"name": "high_school_physics", "score": 0.68, "num": 19},
                                {"name": "high_school_chemistry", "score": 0.21, "num": 19},
                                {"name": "high_school_biology", "score": 0.47, "num": 19},
                                {"name": "middle_school_mathematics", "score": 0.47, "num": 19},
                                {"name": "middle_school_biology", "score": 0.48, "num": 21},
                                {"name": "middle_school_physics", "score": 0.63, "num": 19},
                                {"name": "middle_school_chemistry", "score": 0.7, "num": 20},
                                {"name": "veterinary_medicine", "score": 0.48, "num": 23},
                                {"name": "computer_network", "score": 0.42, "num": 19},
                                {"name": "operating_system", "score": 0.21, "num": 19},
                                {"name": "computer_architecture", "score": 0.52, "num": 21},
                                {"name": "college_programming", "score": 0.43, "num": 37},
                                {"name": "college_physics", "score": 0.11, "num": 19},
                                {"name": "college_chemistry", "score": 0.25, "num": 24},
                                {"name": "advanced_mathematics", "score": 0.16, "num": 19},
                                {"name": "probability_and_statistics", "score": 0.17, "num": 18},
                                {"name": "discrete_mathematics", "score": 0.5, "num": 16}
                            ]
                        },
                        {
                            "name": ["Social Science"],
                            "num": 220,
                            "score": 0.471,
                            "macro_score": 0.471,
                            "subsets": [
                                {"name": "college_economics", "score": 0.53, "num": 55},
                                {"name": "business_administration", "score": 0.39, "num": 33},
                                {"name": "marxism", "score": 0.47, "num": 19},
                                {"name": "mao_zedong_thought", "score": 0.54, "num": 24},
                                {"name": "education_science", "score": 0.52, "num": 29},
                                {"name": "teacher_qualification", "score": 0.7, "num": 44},
                                {"name": "high_school_politics", "score": 0.47, "num": 19},
                                {"name": "high_school_geography", "score": 0.32, "num": 19},
                                {"name": "middle_school_politics", "score": 0.52, "num": 21},
                                {"name": "middle_school_geography", "score": 0.25, "num": 12}
                            ]
                        }
                    ]
                }
            ],
            "analysis": "N/A"
        }
    ],
    "log_path": ""
}
  ]
}

