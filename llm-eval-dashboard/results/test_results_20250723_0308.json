{
    "project_id": "yyz_test_20250722_2000",
    "timestamp": "2025-07-22T12:11:33.460071Z",
    "model_name": "deepseek",
    "run_mode": "acc-then-bench",
    "base_info": {
        "device": "模组",
        "model": "Qwen-1.5B",
        "software_version": 449,
        "version": "1.0.0",
        "software_url": "https://oss.mthreads.com:9001/buckets/release-ci/browse/Y29tcHV0ZVFBL2N1ZGFfY29tcGF0aWJsZS9DSS9yZWxlYXNlX211c2FfNC4xLjA=",
        "vllm_image_harbor": {
            "huoshan_harbor_url": "registry.mthreads.com/mcconline/vllm-musa-qy2-py310:v0.8.4-release",
            "shanghai_harbor_url": "sh-harbor.mthreads.com/vllm-images/vllm-musa-qy2-py310:v0.8.4-release"
        }
    },
    "tests": [
        {
            "test_type": "benchmark",
            "results": [
                {
                    "timestamp": "2025-07-22",
                    "concurrency": 16,
                    "input_tokens": 1270,
                    "output_tokens": 1280,
                    "expected_input": 128,
                    "expected_output": 128,
                    "avg_input": 127.0,
                    "avg_output": 128.0,
                    "total_time": 9.0,
                    "req_throughput": 1.11,
                    "output_throughput": 142.22,
                    "total_throughput": 283.34,
                    "avg_ttft": 540.57,
                    "p99_ttft": 542.55
                },
                {
                    "timestamp": "2025-07-22",
                    "concurrency": 16,
                    "input_tokens": 1270,
                    "output_tokens": 640,
                    "expected_input": 128,
                    "expected_output": 64,
                    "avg_input": 127.0,
                    "avg_output": 64.0,
                    "total_time": 4.7,
                    "req_throughput": 2.13,
                    "output_throughput": 136.13,
                    "total_throughput": 406.26,
                    "avg_ttft": 537.78,
                    "p99_ttft": 539.97
                },
                {
                    "timestamp": "2025-07-22",
                    "concurrency": 32,
                    "input_tokens": 1270,
                    "output_tokens": 1280,
                    "expected_input": 128,
                    "expected_output": 128,
                    "avg_input": 127.0,
                    "avg_output": 128.0,
                    "total_time": 8.99,
                    "req_throughput": 1.11,
                    "output_throughput": 142.37,
                    "total_throughput": 283.63,
                    "avg_ttft": 539.27,
                    "p99_ttft": 541.57
                },
                {
                    "timestamp": "2025-07-22",
                    "concurrency": 32,
                    "input_tokens": 1270,
                    "output_tokens": 640,
                    "expected_input": 128,
                    "expected_output": 64,
                    "avg_input": 127.0,
                    "avg_output": 64.0,
                    "total_time": 4.72,
                    "req_throughput": 2.12,
                    "output_throughput": 135.47,
                    "total_throughput": 404.28,
                    "avg_ttft": 538.15,
                    "p99_ttft": 539.96
                }
            ],
            "log_path": "/home/yuyongzhong/llm-infer/test/output/yyz_test_20250722_2000/benchmark/result/summary_result_20250722_1210.md"
        },
        {
            "test_type": "accuracy",
            "datasets": [
                {
                    "name": "deepseek@ceval",
                    "dataset_name": "ceval",
                    "dataset_pretty_name": "C-Eval",
                    "dataset_description": null,
                    "model_name": "deepseek",
                    "score": 0.825,
                    "metrics": [
                        {
                            "name": "AverageAccuracy",
                            "num": 40,
                            "score": 0.825,
                            "macro_score": 0.8207,
                            "categories": [
                                {
                                    "name": [
                                        "Humanities"
                                    ],
                                    "num": 22,
                                    "score": 0.8636,
                                    "macro_score": 0.8636,
                                    "subsets": [
                                        {
                                            "name": "logic",
                                            "score": 0.8636,
                                            "num": 22
                                        }
                                    ]
                                },
                                {
                                    "name": [
                                        "STEM"
                                    ],
                                    "num": 18,
                                    "score": 0.7778,
                                    "macro_score": 0.7778,
                                    "subsets": [
                                        {
                                            "name": "high_school_mathematics",
                                            "score": 0.7778,
                                            "num": 18
                                        }
                                    ]
                                }
                            ]
                        }
                    ],
                    "analysis": "N/A"
                }
            ],
            "log_path": "/home/yuyongzhong/llm-infer/test/output/yyz_test_20250722_2000/Accuracy_Test/20250722_1154_acc_test.log"
        }
    ]
}